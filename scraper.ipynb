{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from enum import Enum\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "class CitationStyle(Enum):\n",
    "    AMA = \"american-medical-association\"\n",
    "    APSA = \"american-political-science-association\"\n",
    "    APA = \"apa\"\n",
    "    CHICAGO = \"chicagob\"\n",
    "    HARVARD = \"harvard\"\n",
    "    IEEE = \"ieee\"\n",
    "    MHRA = \"modern-humanities-research-association\"\n",
    "    MLA = \"mla7\"\n",
    "    VANCOUVER = \"vancouver\"\n",
    "\n",
    "#To click the cite button, idk but it only works when i use this\n",
    "def click_element(driver, by, value):\n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            element = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((by, value))\n",
    "            )\n",
    "            time.sleep(0.5)\n",
    "            element.click()\n",
    "            break  # Exit the loop if successful\n",
    "        except StaleElementReferenceException:\n",
    "            # print(f\"Attempt {attempt + 1} failed. Retrying...\")\n",
    "            time.sleep(.5)  # Backoff before retrying\n",
    "\n",
    "\n",
    "#Get Abstract and Chicago Citation from link\n",
    "def get_info(link, citation_style):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Enable headless mode\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # For environments where sandboxing is not available\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(link)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        s = soup.find('div', class_='abstract')\n",
    "        abstract = s.text\n",
    "    except Exception as e:\n",
    "        print(\"error at abstract\")\n",
    "        raise e\n",
    "\n",
    "    # print(button)\n",
    "    click_element(driver, By.LINK_TEXT, \"Show author details\")\n",
    "    s = soup.find_all('div', class_='row author')\n",
    "    authors = []\n",
    "\n",
    "    try:\n",
    "        for row in s:\n",
    "            authors.append(row.text.strip().split(' Affiliation: ')[1])\n",
    "    except Exception as e:\n",
    "        print(\"error at authors\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "    click_element(driver, By.CLASS_NAME, \"export-citation-product\")\n",
    "\n",
    "    select_element = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.ID, \"selectCitationStyle\"))\n",
    "                )\n",
    "    \n",
    "    citation = driver.find_element(By.ID, \"citationText\") #Need to put this after selectCitationStyle becomes visible\n",
    "    initial_text = citation.text\n",
    "    \n",
    "    select = Select(select_element)\n",
    "    select.select_by_value(citation_style.value)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            lambda d: citation.text != initial_text\n",
    "        )\n",
    "        citation = citation.text\n",
    "    except Exception as e:\n",
    "        print(\"error at waiting for citation to change\")\n",
    "        raise e\n",
    "\n",
    "    driver.close()\n",
    "    return abstract, citation, authors\n",
    "\n",
    "#Get links of all articles from link\n",
    "\n",
    "\n",
    "def parallel_get_info(link, citation_style):\n",
    "    while True:\n",
    "        try:\n",
    "            abstract, citation, authors = get_info(link, citation_style)\n",
    "            # Create a dictionary with 'Authors' as a list of authors\n",
    "            return {'Chicago Citation': citation, 'Abstract': abstract,  'First Author Institution': authors[0], 'Other Author Institutions': ' / '.join(authors[1:])}\n",
    "        except Exception as e:\n",
    "            print(f\"Error occured, retrying\")\n",
    "\n",
    "def get_pages(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    pages = soup.find('div', class_=\"pagination-centered\")\n",
    "    pages = pages.find('p').text\n",
    "    pages = int(pages[-1])\n",
    "\n",
    "    links = [url]\n",
    "    for i in range(2, pages+1):\n",
    "        links.append(url.replace(\"pageNum=1\", \"pageNum=\" + str(i)))\n",
    "    \n",
    "    return links\n",
    "    \n",
    "\n",
    "def scrape(link, max_workers=10, output_file='output', csv=False, excel=True, citation_style=CitationStyle.CHICAGO):\n",
    "    \"\"\"\n",
    "    Input: link (str), max_workers (int), output_filename (str), csv (bool), excel (bool)\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    links = get_pages(link)\n",
    "    all_links = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tasks to the executor for each link\n",
    "            future_to_link = {executor.submit(get_links, link): link for link in links}\n",
    "            \n",
    "            # As each task completes, append the result to the DataFrame\n",
    "            for future in as_completed(future_to_link):\n",
    "                result = future.result()\n",
    "                if result:  # If result is not None\n",
    "                    all_links.extend(result)\n",
    "\n",
    "    for i in all_links:\n",
    "        print(i)\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Set up a ThreadPoolExecutor for parallel execution\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks to the executor for each link\n",
    "        future_to_link = {executor.submit(parallel_get_info, link, citation_style): link for link in all_links}\n",
    "        \n",
    "        # As each task completes, append the result to the DataFrame\n",
    "        for future in as_completed(future_to_link):\n",
    "            result = future.result()\n",
    "            if result:  # If result is not None\n",
    "                df = pd.concat([df, pd.DataFrame([result])], ignore_index=True)\n",
    "\n",
    "    # Save the final DataFrame to an Excel file\n",
    "    if(excel):\n",
    "        df.to_excel(f'{output_file}.xlsx', index=False)\n",
    "    if(csv):\n",
    "        df.to_csv(f'{output_file}.csv', index=False)\n",
    "\n",
    "\n",
    "# REPLACE LINK WITH THE LINK\n",
    "# LOOK AT THE ORANGE TEXT TO SEE THE PARAMETERS, AND ADD THEM INTO THE scrape(link, csv = True) TO CHANGE OPTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    domain = \"https://www.cambridge.org\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    s = soup.find('h4', class_=\"journal-article-listing-type\")\n",
    "    siblings = [s] + s.findNextSiblings()\n",
    "    s.findnex\n",
    "    result = []\n",
    "    inReviews = False\n",
    "    for sibling in siblings:\n",
    "        if sibling.name == \"h4\":\n",
    "            if sibling.text == \"Book Reviews\":\n",
    "                inReviews = True\n",
    "            else:\n",
    "                inReviews = False\n",
    "            continue\n",
    "        # Don't know what h3 are supposed to represent (its like a name of something), skip book reviews and anything that contains front/back matter\n",
    "        if inReviews or sibling.name == \"h3\" or sibling.text.lower().__contains__(\"front matter\") or sibling.text.lower().__contains__(\"back matter\") or sibling.text.lower().__contains__(\"book review\"):\n",
    "            continue\n",
    "        result.append(sibling)\n",
    "        print(sibling.find('a').text)\n",
    "    siblings = result\n",
    "    links = list(map(lambda x: domain + x.find('a').get('href'), siblings))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "link = \"https://www.cambridge.org/core/journals/american-political-science-review/issue/AA64DAD7A735A935540A462898BF0876?sort=canonical.position%3Aasc&pageNum=1&searchWithinIds=AA64DAD7A735A935540A462898BF0876&productType=JOURNAL_ARTICLE&template=cambridge-core%2Fjournal%2Farticle-listings%2Flistings-wrapper&hideArticleJournalMetaData=true&displayNasaAds=false\"\n",
    "\n",
    "print(get_links(\"https://www.cambridge.org/core/journals/american-political-science-review/issue/AA64DAD7A735A935540A462898BF0876?sort=canonical.position%3Aasc&pageNum=2&searchWithinIds=AA64DAD7A735A935540A462898BF0876&productType=JOURNAL_ARTICLE&template=cambridge-core%2Fjournal%2Farticle-listings%2Flistings-wrapper&hideArticleJournalMetaData=true&displayNasaAds=false\") == None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

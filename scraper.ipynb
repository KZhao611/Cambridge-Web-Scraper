{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab abstract, citation in chicago, unis of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To click the cite button, idk but it only works when i use this\n",
    "def click_element(driver, by, value):\n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Wait for the element to be clickable\n",
    "            element = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((by, value))\n",
    "            )\n",
    "            time.sleep(.5)\n",
    "            element.click()\n",
    "            # print(\"Element clicked successfully!\")\n",
    "            break  # Exit the loop if successful\n",
    "        except StaleElementReferenceException:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Abstract and Chicago Citation from link\n",
    "def get_info(link):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Enable headless mode\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # For environments where sandboxing is not available\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(link)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # print(button)\n",
    "    click_element(driver, By.LINK_TEXT, \"Show author details\")\n",
    "    s = soup.find_all('div', class_='row author')\n",
    "    authors = []\n",
    "    for row in s:\n",
    "        authors.append(row.text.strip().split(' Affiliation: ')[1])\n",
    "\n",
    "    click_element(driver, By.CLASS_NAME, \"export-citation-product\")\n",
    "\n",
    "    select_element = WebDriverWait(driver, 5).until(\n",
    "                    EC.visibility_of_element_located((By.ID, \"selectCitationStyle\"))\n",
    "                )\n",
    "    citation = driver.find_element(By.ID, \"citationText\")\n",
    "    initial_text = citation.text\n",
    "\n",
    "    select = Select(select_element)\n",
    "    select.select_by_value(\"chicagob\")\n",
    "\n",
    "    WebDriverWait(driver, 5).until(\n",
    "        lambda d: citation.text != initial_text\n",
    "    )\n",
    "    citation = citation.text\n",
    "\n",
    "    s = soup.find('div', class_='abstract')\n",
    "    abstract = s.text\n",
    "    \n",
    "    driver.close()\n",
    "    return abstract, citation, authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get links of all articles from link\n",
    "def get_links(url):\n",
    "    domain = \"https://www.cambridge.org\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    s = soup.find('h4', class_=\"journal-article-listing-type\")\n",
    "    siblings = s.findNextSiblings()\n",
    "\n",
    "    result = []\n",
    "    for sibling in siblings:\n",
    "        if sibling.name == \"h4\":\n",
    "            break\n",
    "        result.append(sibling)\n",
    "\n",
    "    siblings = result\n",
    "    links = list(map(lambda x: domain + x.find('a').get('href'), siblings))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_get_info(link):\n",
    "    try:\n",
    "        abstract, citation, authors = get_info(link)\n",
    "        # Create a dictionary with 'Authors' as a list of authors\n",
    "        return {'Chicago Citation': citation, 'Abstract': abstract,  'First Author Institution': authors[0], 'Other Author Institutions': ' / '.join(authors[1:])}\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {link}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(link, max_workers=5, output_file='output', csv=False, excel=True):\n",
    "    \"\"\"\n",
    "    Input: link (str), max_workers (int), output_filename (str), csv (bool), excel (bool)\n",
    "    \"\"\"\n",
    "    links = get_links(link)\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Set up a ThreadPoolExecutor for parallel execution\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks to the executor for each link\n",
    "        future_to_link = {executor.submit(parallel_get_info, link): link for link in links}\n",
    "        \n",
    "        # As each task completes, append the result to the DataFrame\n",
    "        for future in as_completed(future_to_link):\n",
    "            result = future.result()\n",
    "            if result:  # If result is not None\n",
    "                df = pd.concat([df, pd.DataFrame([result])], ignore_index=True)\n",
    "\n",
    "    # Save the final DataFrame to an Excel file\n",
    "    if(excel):\n",
    "        df.to_excel(f'{output_file}.xlsx', index=False)\n",
    "    if(csv):\n",
    "        df.to_csv(f'{output_file}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.cambridge.org/core/journals/american-political-science-review/issue/C8F012F00B0AC2E021E2BC2142FA6AF5?sort=canonical.position%3Aasc&pageNum=1&searchWithinIds=C8F012F00B0AC2E021E2BC2142FA6AF5&productType=JOURNAL_ARTICLE&template=cambridge-core%2Fjournal%2Farticle-listings%2Flistings-wrapper&hideArticleJournalMetaData=true&displayNasaAds=false\"\n",
    "scrape(link, csv=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
